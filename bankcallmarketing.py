# -*- coding: utf-8 -*-
"""BankCallMarketing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PZlzQr9WqmMPyw7E6i8EfOivZyQsPx_v

# 꼭 모두에게 전화를 해야할까
**데이터 기반 맞춤형 텔레마케팅 전략**

## 문제정의

- 은행의 텔레마케팅은 현재 모든 고객에게 동일한 전략(콜 횟수, 채널, 시점)을 적용하고 있음
- 그러나 고객별로 예금 상품에 대한 특성이 다르고 반응도 다를 가능성이 존재
  - 동일 전략은 비용 증가 및 효율 저하를 초래
- 따라서, **고객 특성 기반으로 군집을 구분**하고, 각 군집별로 예금 가입 반응(효율성, 채널, 횟수 등)을 비교·분석하여 **무직위 콜 전략 대비 효율성을 높일 수 있는 고객군별 맞춤 전략**을 제안하는 것이 목표

- '사전 고객 특성'으로 고객을 군집화한 후, 사후에 콜 변수를 매핑하여 군집별 최적 연락 전략을 제안

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""# Import Data"""

df = pd.read_csv('/content/drive/MyDrive/dartb/2025_2/bank_full.csv', sep=';')

df.head()

"""- **age** : 고객 나이
- **job** : 직업 유형 (admin., unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services)
- **marital** : 결혼 상태 (married, divorced(이혼·사별 포함), single)
- **education** : 교육 수준 (unknown, primary, secondary, tertiary)
- **default** : 신용불량 여부 (yes/no)
- **balance** : 평균 연간 잔고, 유로 단위
- **housing** : 주택 대출 여부 (yes/no)
- **loan** : 개인 대출 여부 (yes/no)
- **contact** : 연락 수단 (unknown, telephone, cellular)
- **day** : 마지막 연락 날짜
- **month** : 마지막 연락 월 (jan, feb, …, nov, dec)
- **duration** : 마지막 통화 시간
- **campaign** : 이번 캠페인에서 고객에게 시도한 연락 횟수 (마지막 통화 포함)
- **pdays** : 이전 캠페인 이후 경과 일수 (-1 = 이전에 연락한 적 없음)
- **previous** : 이번 캠페인 전까지의 총 연락 횟수
- **poutcome** : 이전 캠페인의 결과 (unknown, other, failure, success)
- **y** : 정기예금 가입 여부 (yes/no)

# EDA
"""

df.info()

"""- 결측치 없음
- 수치형 변수: age, balance, day, duration, campaign, pdays, previous
- 범주형 변수: job, marital, education, default, housing, loan, contact, month, poutcome, y
"""

df.isna().sum()

df.describe()

"""1. balance
- 최소값이 -8019
- 편차가 크고 중앙값과 평균 차이가 큰 걸 보아 상위 %의 고객의 잔고 값이 극단적

# 수치형 변수
"""

# 수치형 변수 리스트
num_cols = ["age", "balance", "day", "duration", "campaign", "pdays", "previous"]

# 그래프 크기 설정
plt.figure(figsize=(15, 20))

for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 2, 2*i-1)
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"{col} Histogram")

    plt.subplot(len(num_cols), 2, 2*i)
    sns.boxplot(x=df[col])
    plt.title(f"{col} Boxplot")

plt.tight_layout()
plt.show()

"""1. age
- 30~40대 고객이 가장 많음
- 60대 이상 고령자는 소수(이상치로 잡힘)\

2. balance
- 대부분 0~5000, 극단값 많음

3. day
- 일부 날짜에 집중
- 캠페인 타이밍과 성과의 관계 비교 필요

4. duration
- 대부분 0~500, 이상치 존재

5. campaign
- 대부분 1~3
- 반복 연락시 가입 효과가 떨어지는 경항 -> 최적 연락 횟수 도출 필요

6. pdays
- 대부분 이전 연략 없음

7. previous
- 대부분 이전 연락 없음

## balance 확인
"""

# balance 음수인 고객만 추출
neg_balance = df[df['balance'] < 0]

# 전체 음수 balance 고객 수
print("음수 balance 고객 수:", len(neg_balance))

# default / housing / loan 분포 확인
print("\n[default 분포]")
print(neg_balance['default'].value_counts(normalize=True))

print("\n[housing 분포]")
print(neg_balance['housing'].value_counts(normalize=True))

print("\n[loan 분포]")
print(neg_balance['loan'].value_counts(normalize=True))

# 교차표로 동시에 확인
print("\n[교차표]")
print(pd.crosstab(
    index=[neg_balance['default'], neg_balance['housing']],
    columns=neg_balance['loan'],
    margins=True, normalize='all'
))

# 1. default / housing / loan 분포 시각화
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

sns.barplot(x=neg_balance['default'].value_counts(normalize=True).index,
            y=neg_balance['default'].value_counts(normalize=True).values,
            ax=axes[0], palette="Blues")
axes[0].set_title("Default 분포")
axes[0].set_ylabel("비율")

sns.barplot(x=neg_balance['housing'].value_counts(normalize=True).index,
            y=neg_balance['housing'].value_counts(normalize=True).values,
            ax=axes[1], palette="Greens")
axes[1].set_title("Housing dstn")
axes[1].set_ylabel("negative balance %")

sns.barplot(x=neg_balance['loan'].value_counts(normalize=True).index,
            y=neg_balance['loan'].value_counts(normalize=True).values,
            ax=axes[2], palette="Oranges")
axes[2].set_title("Loan 분포")
axes[2].set_ylabel("비율")

plt.tight_layout()
plt.show()


# 2. 교차표 heatmap 시각화
cross_tab = pd.crosstab(
    [neg_balance['default'], neg_balance['housing']],
    neg_balance['loan'],
    normalize='all'
)

plt.figure(figsize=(6, 4))
sns.heatmap(cross_tab, annot=True, fmt=".2f", cmap="Blues")
plt.title("default × housing × loan %")
plt.ylabel("default, housing")
plt.xlabel("loan")
plt.show()

"""- 음수 balance ≠ 데이터 오류/신용불량
- 대출 보유(특히 주택담보)와 연관된 ‘정상적 계좌 상태’로 해석하는 게 타당

1. 연체와 무관
- 음수 balance 고객 중 default=no가 88.3% → 잔고가 음수라도 연체 상태가 아님을 시사.

2. 대출과의 구조적 연관
- housing=yes가 72.7%로 다수 → 주택담보대출 보유자에서 음수 balance가 흔함
- loan(개인대출)은 no가 67.9% → 음수 balance가 개인대출 보유와 1:1 대응하지 않음(주택담보 중심)
3. 교차표 패턴 일관성
- 가장 큰 집단: default=no & housing=yes & loan=no (47.6%)
- 즉, 연체는 없고 주담대는 있으나 개인대출은 없는 고객이 음수 balance의 핵심 집단\
→ 마이너스 통장/당좌대월/상환 스케줄 등으로 허용된 범위의 일시적 음수일 가능성이 높음.
4. 도메인 논리
- 은행은 허용 한도 내 음수(마이너스 통장, 오버드래프트)를 정상 거래로 본다.
- 따라서 “잔고 음수 → 신용불량” 가정이 성립하지 않음.

## 파생변수 생성

**pdays/previous 이진변수**
"""

# pdays / previous 관련 파생변수 생성

# 1. 과거 캠페인에서 연락한 적 있는지 여부
df["contacted_before"] = (df["pdays"] != -1).astype(int)

# 2. 과거 캠페인에서 성공 경험이 있는지 여부
df["effective_previous"] = ((df["previous"] > 0) & (df["poutcome"] == "success")).astype(int)

df.head()

"""- contacted_before: 과거 캠페인에서 연락한 적 있는지 여부
- effective_previous: 과거 캠페인에서 성공 경험이 있는지 여부

**campaign**
"""

# 95% 지점 계산
campaign_95 = df["campaign"].quantile(0.95)
print("campaign 95th percentile cutoff:", campaign_95)

# 1. high_campaign: 95% 초과 여부
df["high_campaign"] = (df["campaign"] > campaign_95).astype(int)

# 2. campaign_capped: 상한 제한 (95% 초과 값은 cutoff 값으로 대체)
df["campaign_capped"] = df["campaign"].clip(upper=campaign_95)

df.head()

"""**balance**"""

# balance 음수 여부 파생변수 생성
df["balance_negative"] = (df["balance"] < 0).astype(int)

# 확인
df.head()

"""**y**"""

df["y_bin"] = (df["y"].str.lower() == "yes").astype("int8")

"""# 수치형 변수의 상관관계"""

# 상관계수 계산
corr = df[num_cols].corr()

# 히트맵 시각화
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True)
plt.title("Correlation Heatmap of Numeric Variables", fontsize=14)
plt.show()

"""- 대부분의 상관계수가 -0.1 ~ 0.1 수준 → 수치형 변수들 간에 뚜렷한 선형 상관관계는 거의 없음.
- 다중공선성(multicollinearity) 문제는 크지 않아 보임.

# 범주형 변수 간의 관계 분석
"""

from scipy.stats import chi2_contingency

# Cramér’s V 계산 함수
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    r, k = confusion_matrix.shape
    return np.sqrt(chi2 / (n * (min(r, k) - 1)))

# 범주형 변수 리스트
cat_cols = [
    "job", "marital", "education", "default", "housing",
    "loan", "contact", "month", "poutcome",
    "contacted_before", "effective_previous", "high_campaign"
]

# 결과 저장
cramers_results = pd.DataFrame(index=cat_cols, columns=cat_cols)

for col1 in cat_cols:
    for col2 in cat_cols:
        if col1 == col2:
            cramers_results.loc[col1, col2] = 1.0
        else:
            cramers_results.loc[col1, col2] = cramers_v(df[col1], df[col2])

# 수치형 변환
cramers_results = cramers_results.astype(float)

# 히트맵 시각화
plt.figure(figsize=(12,10))
sns.heatmap(
    cramers_results,
    annot=True,
    fmt=".2f",
    cmap="Blues",
    cbar=True
)
plt.title("Cramér's V Heatmap (Categorical + Binary Variables)", fontsize=14)
plt.show()

"""1. education & job (0.46)
- 직업과 교육이 중간 정도 연관 있음

2. housing & month (0.50)
- 주택담보대출 보유 여부와 캠페인 월이 중간 수준의 상관관계가 있음
- 왜그러지

3. contact & month (0.51)
- 연락 방식과 월 간에 중간 수준의 상관관계
- 특정 달에 특정 접촉 방식이 주로 쓰였을 걸로 예상

4. contact & contacted_before (0.29)
- 연락 방식과 과거 접촉 여부가 약한 상관관계

# y 분포 확인
"""

# 막대 그래프 시각화
plt.figure(figsize=(6,4))
sns.countplot(x="y", data=df, palette="Set2")
plt.title("Distribution of Subscription (y)")
plt.xlabel("Subscription (y)")
plt.ylabel("Count")
plt.show()

"""## 수치형 변수 vs y"""

num_cols = [
    "age", "balance", "day", "duration", "campaign", "pdays", "previous",
    "balance_negative", "contacted_before", "effective_previous", "high_campaign",
    "campaign_capped"
]

plt.figure(figsize=(15, 20))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols)//2 + 1, 2, i)
    sns.boxplot(x="y", y=col, data=df)
    plt.title(f"{col} by y")
plt.tight_layout()
plt.show()

from scipy.stats import mannwhitneyu

# y에 따라 그룹 나누기
pdays_no = df[df["y"]=="no"]["pdays"]
pdays_yes = df[df["y"]=="yes"]["pdays"]

stat, p = mannwhitneyu(pdays_no, pdays_yes, alternative="two-sided")
print("Mann-Whitney U 통계량 =", stat)
print("p-value =", p)

"""**주요 차이점**
1. duration (통화 시간)
- 가입 고객(y=yes)의 통화 시간이 훨씬 김  
- 가장 강력한 설명 변수

2. campaign (연락 횟수)
- 가입 고객은 오히려 연락 횟수가 적음  
- 과도한 연락은 역효과

3. pdays / contacted_before (과거 접촉 경험)
- 최근에 다시 연락받은 경우, 과거에 연락 경험이 있는 경우 가입률 ↑  
- 마케팅 이력 변수가 가입 확률에 중요한 영향

**영향이 미약한 변수**
1. balance: 가입 여부와 큰 차이 없음  
2. age: 두 그룹 모두 전 연령대에 분포, 뚜렷한 패턴 없음  
3. previous / effective_previous: 약간 차이는 있으나 극단치 영향 큼  
4. balance_negative / high_campaign / campaign_capped / campaign_log: 가입 여부와 뚜렷한 차이 없음

## 범주형 변수 vs y
"""

# 범주형 변수 리스트
cat_cols = [
    "job", "marital", "education", "default", "housing",
    "loan", "contact", "month", "poutcome",
    "contacted_before", "effective_previous", "high_campaign"
]

plt.figure(figsize=(15, 25))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(len(cat_cols)//2 + 1, 2, i)
    rate = pd.crosstab(df[col], df["y"], normalize="index")["yes"]
    rate.plot(kind="bar", ax=plt.gca(), color="skyblue")
    plt.title(f"Subscription Rate by {col}")
    plt.ylabel("Proportion of Yes")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""1. Job
- 학생(student), 은퇴(retired), 관리직(management)에서 가입률이 높음
- 블루칼라(blue-collar), 자영업(self-employed) 등은 낮음

2. Marital
- single > divorced > married 순으로 가입률이 높음

3. Education
- tertiary(고등 교육)에서 가장 높은 가입률
- primary(초등 교육)에서 가장 낮음
  -> 교육 수준이 높을수록 예금 상품에 대한 관심이 높음

4. Default/Housing/Loan
- default=no 그룹이 yes 그룹보다 가입률이 높음
- housing=no 그룹이 yes 그룹보다 가입률이 높음
- loan=no 그룹이 loan=yes보다 가입률이 높음
  - **대출이 없는 고객이 일관되게 더 높음**

5. Contact
- cellular > telephone > unknown 순
- 휴대전화로 연락 시 가입률 ↑

6. Month
- 3월, 9월, 10월, 12월에서 가입률이 상대적으로 높음
- 여름철(5~8월)은 낮음

7. Poutcome (이전 캠페인 결과)
- success 그룹은 가입률이 매우 높음(0.6 이상)
- failure, other, unknown은 모두 낮음
  - 과거에 캠페인에 긍정적이었던 고객은 다시 가입할 확률이 높음

8. Age_band
- 60세 이상에서 가입률이 급격히 증가
  - 고령자층이 안정적 예금 상품에 관심이 많음
- student의 응답률이 높은데 왜 age에서는???

9. contacted_before
- 과거에 한 번이라도 연락을 받은 고객의 가입률이 2배 이상 높음
  - **재접촉 고객은 타겟팅 우선순위**

10. effective_previous
- 과거 캠페인 성공 경험은 고객은 가입확률이 높음(65%)
  - CRM 데이터에 가장 강력한 타겟팅 기준

11. high_compain
- 너무 자주(7회 초과) 연락하면 오히려 가입률이 떨어짐
  - 연락 횟수 최적화 필요



**종합 결론**
1. **인구통계 변수**: 나이↑, 교육↑, 미혼(single) → 가입률↑  
2. **대출 여부 변수**: 대출 없음(no default, no housing, no loan) → 가입률↑  
3. **캠페인 변수**: 휴대전화(contact=cellular), 이전 캠페인 성공(poutcome=success), 특정 시기(3월, 9월, 12월) → 가입률↑

# 교차분석

## 대출여부 x 가입여부
"""

# 각 대출 변수와 y 교차표 (비율 포함)
for col in ["housing", "loan", "default"]:
    print(f"\n=== {col} × y 교차분석 ===")

    # 빈도표
    crosstab_freq = pd.crosstab(df[col], df["y"])
    print("\n빈도표:")
    print(crosstab_freq)

    # 비율표 (행 기준 → 각 그룹 내 yes 비율)
    crosstab_rate = pd.crosstab(df[col], df["y"], normalize="index").round(3)
    print("\n비율표 (행 기준):")
    print(crosstab_rate)

# 3개 조건을 동시에 만족하는 그룹 만들기
df["no_debt_group"] = (
    (df["housing"] == "no") &
    (df["loan"] == "no") &
    (df["default"] == "no")
).astype(int)

# 교차표: no_debt_group × y
crosstab_freq = pd.crosstab(df["no_debt_group"], df["y"])
crosstab_rate = pd.crosstab(df["no_debt_group"], df["y"], normalize="index").round(3)

print("=== 빈도표 ===")
print(crosstab_freq)
print("\n=== 비율표 (행 기준) ===")
print(crosstab_rate)

"""- housing, loan, default 모두 no인 고객 가입률 18.4%
- 하나라도 있는 고객 가입률 7.7%
  - 두 배 이상 높음 -> 전체 표본에서도 적은 수가 아닌데 가입 효율이 높음

## Contact x Month
"""

# Contact × Month 교차표 (빈도)
crosstab_freq = pd.crosstab(df["contact"], df["month"])
print("=== 빈도표 ===")
print(crosstab_freq)

# Contact × Month 교차표 (비율, 행 기준 → 연락 방식별 월 분포)
crosstab_pct_row = pd.crosstab(df["contact"], df["month"], normalize="index").round(3)
print("\n=== 비율표 (행 기준) ===")
print(crosstab_pct_row)

# Contact × Month 교차표 (비율, 열 기준 → 월별 연락 방식 분포)
crosstab_pct_col = pd.crosstab(df["contact"], df["month"], normalize="columns").round(3)
print("\n=== 비율표 (열 기준) ===")
print(crosstab_pct_col)

# Contact × Month × y (가입률)
crosstab_y = pd.crosstab([df["contact"], df["month"]], df["y"], normalize="index").round(3)
print("\n=== Contact × Month별 가입률 ===")
print(crosstab_y)

# Contact×Month별 yes 비율만 추출
plot_df = crosstab_y['yes'].reset_index()          # cols: ['contact','month','yes']
plot_df = plot_df.rename(columns={'yes':'subscription_rate'})

# 월 순서 고정
month_order = ["jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"]
plot_df['month'] = pd.Categorical(plot_df['month'], categories=month_order, ordered=True)

# 꺾은선 그래프
plt.figure(figsize=(12,6))
sns.lineplot(data=plot_df, x="month", y="subscription_rate", hue="contact", marker="o")
plt.title("Subscription Rate by Contact and Month")
plt.xlabel("Month")
plt.ylabel("Subscription Rate (Proportion of Yes)")
plt.legend(title="Contact")
plt.tight_layout()
plt.show()

"""# 가설검정

### 1. 이전 연락 시점과 횟수에 따라 가입률에 차이가 있다.

- H₀: 이전 연락으로부터 지난 일수(pdays)와 과거 접촉 횟수(previous)는 가입 여부(y)에 영향을 주지 않는다
- H₁: pdays와 previous의 구간 조합에 따라 가입률이 유의하게 달라진다
"""

from scipy.stats import chi2_contingency

# previous 범주화 -> previous_group
def categorize_previous(x):
    if pd.isna(x):
        return "unknown"
    x = int(x)
    if x == 0:
        return "no_previous(0)"
    elif 1 <= x <= 2:
        return "few_previous(1~2)"
    elif 3 <= x <= 5:
        return "medium_previous(3~5)"
    else:
        return "many_previous(6+)"

df["previous_group"] = df["previous"].apply(categorize_previous)

prev_order = ["no_previous(0)", "few_previous(1~2)", "medium_previous(3~5)", "many_previous(6+)", "unknown"]
df["previous_group"] = pd.Categorical(df["previous_group"], categories=prev_order, ordered=True)

# pdays 도메인 구간 -> pdays_group_domain
def bucket_pdays_domain(x):
    if x == -1:
        return "no_contact"
    elif 0 <= x <= 30:
        return "0-30"
    elif 31 <= x <= 90:
        return "31-90"
    elif 91 <= x <= 180:
        return "91-180"
    else:
        return "181+"

df["pdays_group_domain"] = df["pdays"].apply(bucket_pdays_domain)
pdays_order = ["no_contact", "0-30", "31-90", "91-180", "181+"]
df["pdays_group_domain"] = pd.Categorical(df["pdays_group_domain"], categories=pdays_order, ordered=True)

# 카이제곱 독립성 검정 (pdays×previous vs y)
ct = pd.crosstab([df["pdays_group_domain"], df["previous_group"]], df["y"])
print("교차표(카운트):\n", ct)

chi2, p, dof, expected = chi2_contingency(ct)
print("\n[카이제곱 독립성 검정]")
print(f"Chi2 = {chi2:.3f}, dof = {dof}, p-value = {p:.6f}")

# 참고: 각 조합의 가입률 테이블(평균)
df["y_num"] = (df["y"] == "yes").astype(int)
rate_table = df.pivot_table(index=["pdays_group_domain","previous_group"], values="y_num", aggfunc="mean")
print("\n조합별 가입률(mean):\n", (rate_table*100).round(1))

pdays_order = ["no_contact", "0-30", "31-90", "91-180", "181+"]
prev_order  = ["no_previous(0)", "few_previous(1~2)", "medium_previous(3~5)", "many_previous(6+)"]

df["pdays_group_domain"] = pd.Categorical(df["pdays_group_domain"], categories=pdays_order, ordered=True)
df["previous_group"]     = pd.Categorical(df["previous_group"], categories=prev_order, ordered=True)
df["y_num"] = (df["y"] == "yes").astype(int)

rate_tbl = (
    df.pivot_table(index="pdays_group_domain", columns="previous_group", values="y_num", aggfunc="mean")
    .loc[pdays_order, prev_order] * 100
).round(1)

rate_long = rate_tbl.reset_index().melt(id_vars="pdays_group_domain", var_name="previous_group", value_name="rate")
plt.figure(figsize=(9, 5.2))
sns.barplot(
    data=rate_long,
    x="pdays_group_domain", y="rate", hue="previous_group",
    order=pdays_order, hue_order=prev_order
)
plt.ylabel("Subscription Rate (%)")
plt.xlabel("pdays (domain buckets)")
plt.title("Subscription Rate by pdays bucket and previous group")
plt.legend(title="previous_group", bbox_to_anchor=(1.01,1), loc="upper left")
plt.tight_layout()
plt.show()

"""- p = 0.00 -> 과거연락으로지난시점&횟수는 가입률에 영향을 미친다

# 클러스터링 전 전처리

## unknown 처리
"""

print("=== education ===")
print(df["education"].value_counts(normalize=True).round(3))

print("\n=== poutcome ===")
print(df["poutcome"].value_counts(normalize=True).round(3))

print("\n=== contact ===")
print(df["contact"].value_counts(normalize=True).round(3))

print("\n=== job ===")
print(df["job"].value_counts(normalize=True).round(3))

"""### poutcome

**poutcome에서 unknown은 이전에 접촉한 경험이 없는 고객 아닐까??**\
-> poutcome과 contacted_before의 상관관계 비교(둘이 강한 상한관계를 가질 수록 앞서 가정한 것이 옳을 확률이 높아짐)
"""

# 교차표 생성: poutcome × contacted_before==0 여부
df["pdays_no_contact"] = (df["contacted_before"] == 0).astype(int)

crosstab = pd.crosstab(df["poutcome"], df["pdays_no_contact"], normalize="index").round(3)
print(crosstab)

"""- poutcome = unknown은 전부 pdays_no_contact = 1
  - 따라서, 확실히 이전 캠페인 접촉 경험이 없는 고객의 데이터라고 판단
"""

# 1. unknown을 no_contact로 변경
df["poutcome"] = df["poutcome"].replace("unknown", "no_contact")

# 2. 원-핫 인코딩 (정수형 반환)
poutcome_dummies = pd.get_dummies(df["poutcome"], prefix="poutcome", dtype=int)

# 3. success 여부 파생변수 생성
df["poutcome_success"] = (df["poutcome"] == "success").astype(int)

# 4. 원본 df에 원-핫 결과 붙이기
df = pd.concat([df, poutcome_dummies], axis=1)

# 결과 확인
df.head()

"""- 클러스터링엔 원핫인코딩 전체 넣고 해석할 때는 poutcome_success 변수를 활용(이 클러스터는 success 비율이 몇 %인가?라는 지표)

### edcuation
"""

from scipy.stats import chi2_contingency

# 1. education unknown 여부 플래그
df["edu_unknown_flag"] = (df["education"] == "unknown").astype(int)

# 2. 교차표 (응답률)
crosstab = pd.crosstab(df["edu_unknown_flag"], df["y"])
print("=== 빈도표 ===")
print(crosstab, "\n")

crosstab_norm = pd.crosstab(df["edu_unknown_flag"], df["y"], normalize="index")
print("=== 비율표 (응답률) ===")
print(crosstab_norm, "\n")

# 3. 카이제곱 검정
chi2, p, dof, expected = chi2_contingency(crosstab)
print("=== 카이제곱 검정 ===")
print(f"Chi2: {chi2:.4f}, p-value: {p:.4f}")

"""- 걍 냅두자 유의미하다

### job
"""

from scipy.stats import chi2_contingency

# job unknown 여부 플래그 생성
df["job_unknown_flag"] = (df["job"] == "unknown").astype(int)

# 교차표 생성
crosstab = pd.crosstab(df["job_unknown_flag"], df["y"])
print("=== 빈도표 ===")
print(crosstab)

# 비율표 (응답률)
ratio_table = pd.crosstab(df["job_unknown_flag"], df["y"], normalize="index").round(4)
print("\n=== 비율표 (응답률) ===")
print(ratio_table)

# 카이제곱 검정
chi2, p, dof, expected = chi2_contingency(crosstab)
print("\n=== 카이제곱 검정 ===")
print(f"Chi2: {chi2:.4f}, p-value: {p:.4f}")

"""유의미하지도 않고 애초에 unknown인 행도 0.6인가 그럼 걍 행 삭제 ㄱ"""

# job이 unknown인 행 삭제
df = df[df["job"] != "unknown"].copy()

# 결과 확인
print(df["job"].value_counts(normalize=True))

"""### contact"""

# contact 컬럼 원핫 인코딩
contact_dummies = pd.get_dummies(df["contact"], prefix="contact", dtype=int)

# 기존 df에 합치기
df = pd.concat([df, contact_dummies], axis=1)

# 확인
print(df.head())

"""## 수치형 변수

- balance, campaign, age -> 스케일링(캠페인은 이미 함)
- pdays, previous -> 파생변수만 사용
"""

from sklearn.preprocessing import StandardScaler

# StandardScaler 객체 생성
scaler = StandardScaler()

# age 컬럼만 스케일링
df["age_scaled"] = scaler.fit_transform(df[["age"]])

# 확인
print(df[["age", "age_scaled"]].head())

from sklearn.preprocessing import RobustScaler

# RobustScaler - balance
scaler_robust = RobustScaler()
df["balance_scaled"] = scaler_robust.fit_transform(df[["balance"]])

# 결과 확인
print(df[["balance", "balance_scaled"]].head())

"""- age: 딱히 극단값 없어서 걍 standardscaler 적용
- campaign
  - 극단적으로 치우쳐 있지만 대부분 값이 1~3에 몰려있음
  - log 변환으로 꼬리를 줄이면 분포가 비교적 정규분포에 가까워져서 standardscaler 적용

- balance
  - 음수 포함, 분산이 크고 극단값이 매우 많음
  - 로그변환보다 중앙값과 IQR을 기준으로 스케일링하는 robustscaler 사용하여 극단값의 영향을 최소화

## 범주형 변수

- job, marital, education, contact, month, poutcome
- default, housing, loan
  - contact이랑 poutcome은 이미 함

### job

- 직업군을 일반적인 도메인상으로 먼저 매핑함
- 그 후 매핑 사용 가능한지 검정
"""

job_map = {
    # white_collar
    "management": "white_collar",
    "admin.": "white_collar",
    "administration": "white_collar",  # 혹시 있을 변형
    "professional": "white_collar",
    "technician": "white_collar",

    # blue_collar
    "blue-collar": "blue_collar",
    "services": "blue_collar",
    "service": "blue_collar",          # 변형 대비

    # self_employed
    "self-employed": "self_employed",
    "entrepreneur": "self_employed",

    # low_income_other
    "housemaid": "low_income_other",
    "unemployed": "low_income_other",
    "unknown": "low_income_other",

    # standalone
    "student": "student",
    "retired": "retired",
}

df["job_grouped"] = df["job"].map(job_map).fillna("low_income_other")
df.head()

# 2) 원본 vs 그룹화 가입률 표 (행 기준 비율)
orig_rate = pd.crosstab(df["job"], df["y"], normalize="index").rename(columns={"yes":"rate_yes"}).round(3)
grp_rate  = pd.crosstab(df["job_grouped"], df["y"], normalize="index").rename(columns={"yes":"rate_yes"}).round(3)

print("=== 원본 job별 가입률 ===")
print(orig_rate[["rate_yes"]].sort_values("rate_yes", ascending=False))
print("\n=== 그룹화 job_grouped별 가입률 ===")
print(grp_rate[["rate_yes"]].sort_values("rate_yes", ascending=False))

# 3) 카이제곱 검정 (원본 vs 그룹화)
orig_ct = pd.crosstab(df["job"], df["y"])
grp_ct  = pd.crosstab(df["job_grouped"], df["y"])

chi2_o, p_o, dof_o, _ = chi2_contingency(orig_ct)
chi2_g, p_g, dof_g, _ = chi2_contingency(grp_ct)

print("\n=== 카이제곱 검정: 원본 job × y ===")
print(f"chi2={chi2_o:.2f}, dof={dof_o}, p-value={p_o:.4g}")
print("=== 카이제곱 검정: 그룹화 job_grouped × y ===")
print(f"chi2={chi2_g:.2f}, dof={dof_g}, p-value={p_g:.4g}")

# 4) 그룹화로 인한 정보 손실(이질성) 확인:
#    각 그룹 안에서 원본 job들의 가입률 산포(최소/최대/표준편차) 보이기
tmp = (
    df.assign(_rate=df.groupby("job")["y"].transform(lambda s: (s=="yes").mean()))
      .drop_duplicates(subset=["job"])  # 각 job별 rate만 남기기
      .assign(job_grouped=lambda x: x["job"].map(job_map))
)

hetero = (
    tmp.groupby("job_grouped")["_rate"]
       .agg(n_jobs="count", min_rate="min", max_rate="max", std_rate="std")
       .sort_values("std_rate", ascending=False)
       .round(3)
)

print("\n=== 그룹 내부 이질성(원본 job 가입률 분포) ===")
print(hetero)

# 5) 원본 ↔ 그룹화 매핑과 각 job의 가입률을 한 번에 보는 테이블 (리뷰용)
orig_with_group = (
    orig_rate[["rate_yes"]]
      .rename(columns={"rate_yes":"job_rate_yes"})
      .assign(job_grouped=lambda x: x.index.map(job_map))
      .sort_values(["job_grouped","job_rate_yes"], ascending=[True, False])
)
print("\n=== 원본 job → 그룹 매핑 & 각 job 가입률 ===")
print(orig_with_group)

"""1. 해석력 상승
- white_collar, blue_collar, self_employed, low_income_other, student, retired
- 사회적으로 직관적
- 은행 입장에서 “어떤 직군이 위험/기회 요인인지” 설명이 잘 됨

2. 이질성 확인
- 그룹 내부 std_rate가 대부분 작아 같은 그룹 안에 묶인 직업들이 가입률 분포가 크게 튀지 않음.
  - 내부 동질성도 꽤 확보됨.

3. 통계적 유의성 유지
- 카이제곱 검정 p-value 여전히 매우 작음(≈ 2.446e-162).
  - 그룹핑을 해도 여전히 job ↔ 가입률 관계는 유의미.
"""

# job_grouped 컬럼 원핫 인코딩
job_grouped_dummies = pd.get_dummies(df["job_grouped"], prefix="job_grouped", dtype=int)

# 기존 df에 합치기
df = pd.concat([df, job_grouped_dummies], axis=1)

# 확인
print(df.head())

"""### marital"""

plt.figure(figsize=(8,6))
sns.countplot(data=df, x="marital", hue="y", palette="Set2")

plt.title("Marital Status vs Subscription (count)", fontsize=14)
plt.xlabel("Marital Status")
plt.ylabel("Count")
plt.legend(title="Subscribed (y)")
plt.show()

df = df.drop(columns=[c for c in df.columns if c.startswith("marital_")], errors="ignore")

# 1) marital 컬럼 정리
df["marital"] = df["marital"].astype(str).str.strip().str.lower()

# 2) married vs not_married 구분
df["marital_bin"] = df["marital"].map(
    lambda x: "married" if x == "married"
              else ("not_married" if x in ["single","divorced"] else np.nan)
)

# 3) 원-핫 인코딩 (drop_first=True → 더미 하나만 생성)
marital_dummies = pd.get_dummies(df["marital_bin"], prefix="marital", drop_first=True, dtype=int)

# 4) df에 병합
df = pd.concat([df, marital_dummies], axis=1)

# 결과 확인
df.head()

"""### education"""

# 서열 인코딩
edu_map = {"primary": 1, "secondary": 2, "tertiary": 3}
df["education_encoded"]  = df["education"].map(edu_map).astype("float")   # unknown -> NaN
df["edu_unknown_flag"]   = (df["education"] == "unknown").astype("int8")  # 0/1 플래그

# 2) NaN 대체(중간값으로). 성능/왜곡 균형 좋음
df["education_encoded"].fillna(2, inplace=True)   # secondary로 채움

# 3) 확인
print(df[["education", "education_encoded", "edu_unknown_flag"]].head())
assert not df["education_encoded"].isna().any()

"""### default/loan/housing"""

binary_cols = ["default", "loan", "housing"]

for col in binary_cols:
    df[col] = df[col].map({"yes": 1, "no": 0})

df.head()

"""**파생변수 생성**
- num_loans: 보유한 대출, 부채 개수(housing+loan+default)
"""

# 파생변수 생성
df["num_loans"] = df[["housing", "loan", "default"]].sum(axis=1)

# 스케일링
scaler = StandardScaler()
df["num_loans_scaled"] = scaler.fit_transform(df[["num_loans"]])

"""# 클러스터링"""

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

RANDOM_STATE = 42

feature_cols = [
    "age_scaled", "balance_scaled", "num_loans_scaled",
    "job_grouped_low_income_other", "job_grouped_blue_collar", "job_grouped_retired",
    "job_grouped_self_employed", "job_grouped_student", "job_grouped_white_collar",
    "marital_not_married",
    "education_encoded", "edu_unknown_flag",
]

X = df[feature_cols].copy()

# PCA (설명분산 80%)
pca = PCA(n_components=0.80, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X)
print(f"[PCA] n_components={pca.n_components_}, explained_var_ratio={pca.explained_variance_ratio_.sum():.3f}")

# k 선택: 지표 비교 (wcss 추가)
def evaluate_k(X_mat, k_list):
    rows = []
    for k in k_list:
        km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init="auto")
        labels = km.fit_predict(X_mat)
        sil = silhouette_score(X_mat, labels)
        ch  = calinski_harabasz_score(X_mat, labels)
        db  = davies_bouldin_score(X_mat, labels)
        wcss = km.inertia_   # <- inertia 값 (WCSS) 추가
        rows.append((k, sil, ch, db, wcss))
        print(f"k={k:2d} | silhouette={sil:6.3f} | CH={ch:10.1f} | DB={db:6.3f} | WCSS={wcss:,.1f}")
    return pd.DataFrame(rows, columns=["k","silhouette","calinski_harabasz","davies_bouldin","wcss"])

k_grid = range(2, 7)
metrics_df = evaluate_k(X_pca, k_grid)

# 시각화
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# 실루엣 점수
axes[0].plot(metrics_df["k"], metrics_df["silhouette"], marker="o", label="silhouette")
axes[0].set_xlabel("k"); axes[0].set_ylabel("silhouette score"); axes[0].set_title("Silhouette vs k")
axes[0].grid(True); axes[0].legend()

# 엘보우 메소드 (WCSS)
axes[1].plot(metrics_df["k"], metrics_df["wcss"], marker="o", color="orange", label="WCSS (inertia)")
axes[1].set_xlabel("k"); axes[1].set_ylabel("WCSS"); axes[1].set_title("Elbow Method (WCSS vs k)")
axes[1].grid(True); axes[1].legend()

plt.tight_layout()
plt.show()

"""- 실루엣스코어: 0.538
- 인구통계적 특성은 본질적으로 집단 간 경계가 뚜렷하지 않음에도 불구하고, 비교적 준수한 스코어가 나옴
"""

# 최종 k로 학습 (필요에 맞게 수정)

BEST_K = 3  # <- metrics 출력 보고 조정
kmeans_final = KMeans(n_clusters=BEST_K, random_state=RANDOM_STATE, n_init="auto")
cluster_labels = kmeans_final.fit_predict(X_pca)

df["cluster"] = cluster_labels
print("\n[Clusters] 분포")
print(df["cluster"].value_counts().sort_index())

# 클러스터 프로파일링

def profile_clusters(df, cluster_col="cluster",
                     num_keep=["age_scaled","balance_scaled","num_loans",
                               "campaign","pdays","previous"],  # 수치형 평균 낼 것들
                     dummy_prefixes=("job_group_","marital_","contact_","poutcome_"),
                     cat_keep=["month"],   # 추가로 원본 범주형
                     target_col="y",
                     top_cols=8):
                   # 범주 테이블이 넓으면 상위 n개만
    # 0) 중복 컬럼 제거 & 라벨 있는 행만
    d = df.loc[:, ~df.columns.duplicated()].copy()
    d = d.loc[d[cluster_col].notna()].copy()

    # 1) 더미 → 숫자(0/1) 강제 → 라벨 생성
    gen_labels = []
    for prefix in (dummy_prefixes or []):
        cols = [c for c in d.columns if c.startswith(prefix)]
        if not cols:
            continue
        M = d[cols].apply(pd.to_numeric, errors="coerce").fillna(0)
        M = (M > 0).astype(int)                     # 0/1로 통일
        idx = M.to_numpy().argmax(axis=1)
        lab = pd.Series([cols[i] for i in idx], index=M.index)
        lab[M.sum(axis=1) == 0] = f"{prefix}none"
        name = prefix.rstrip("_") + "_label"
        d[name] = lab.str.replace(prefix, "", regex=False).astype("category")
        gen_labels.append(name)

    # 2) 타깃 변환
    if target_col in d.columns:
        d["y_bin"] = (d[target_col].astype(str).str.strip().str.lower() == "yes").astype(int)

    # 3) 숫자 평균
    num_keep = [c for c in (num_keep or []) if c in d.columns and pd.api.types.is_numeric_dtype(d[c])]
    out = {}
    if num_keep:
        num_tbl = d.groupby(cluster_col)[num_keep].mean().round(3)
        out["numeric_means"] = num_tbl
        print("\n[Numeric means]\n", num_tbl)

    # 4) 범주 분포
    out["cat_tables"] = {}
    cats = [c for c in list(cat_keep or []) + gen_labels if c in d.columns]
    for c in cats:
        ct = pd.crosstab(d[cluster_col], d[c], normalize="index").round(3)
        if top_cols and ct.shape[1] > top_cols:
            keep = ct.sum(0).sort_values(ascending=False).head(top_cols).index
            ct = ct[keep]
        out["cat_tables"][c] = ct
        print(f"\n[{c} 분포 비율]\n", ct)

    # 5) 가입률
    if "y_bin" in d.columns:
        y_tbl = d.groupby(cluster_col)["y_bin"].mean().round(3)
        out["y_rate"] = y_tbl
        print("\n[Cluster별 가입률(y=1)]\n", y_tbl)

    return out

# 숫자/이진 평균으로 볼 컬럼(있는 것만 자동 필터)
_default_num = [
    "age","balance","campaign", "high_campaign", "campaign_capped", "balance_negative", "num_loans",
    "effective_previous","pdays_no_contact","loan","housing","no_debt_group"
]
num_keep = [c for c in _default_num if c in df.columns]

profile_clusters(
    df,
    cluster_col="cluster",
    num_keep=num_keep,  # ← 여기 이제 정의됨
    dummy_prefixes=("job_grouped_","marital_","contact_","poutcome_"),
    cat_keep=("month",),
    target_col="y",
    top_cols=8
)

def plot_clusters_scatter(X_pca, labels, kmeans=None, sample_max=20000, s=8, alpha=0.5):
    """
    X_pca : PCA 좌표 (n x d), d>=2
    labels: 클러스터 라벨 (n,)
    kmeans: KMeans/MiniBatchKMeans 객체(선택). 있으면 중심점 표시
    sample_max: 너무 크면 임의추출로 속도/가독성 확보
    """
    X2 = X_pca[:, :2]  # PC1, PC2만 사용
    labels = np.asarray(labels)

    # (옵션) 샘플링
    n = len(labels)
    if (sample_max is not None) and (n > sample_max):
        rng = np.random.RandomState(42)
        idx = rng.choice(n, sample_max, replace=False)
        X2 = X2[idx]
        labels = labels[idx]

    # 산점도
    plt.figure(figsize=(7,5))
    for c in np.unique(labels):
        m = labels == c
        plt.scatter(X2[m, 0], X2[m, 1], s=s, alpha=alpha, label=f"cluster {int(c)}")

    plt.xlabel("PC1"); plt.ylabel("PC2")
    plt.title("Clusters on PCA space")
    plt.legend()
    plt.show()

# 사용 예시
plot_clusters_scatter(
    X_pca=X_pca,
    labels=df["cluster"].values,   # 또는 cluster_labels
    kmeans=kmeans_final,           # 없으면 None
    sample_max=20000               # 전체 다 보고 싶으면 None
)

"""**1. Cluster 0** (4,429명, 9.5%)

- 재무/고객 특성
  - 평균 나이: 44.1세  
  - 평균 잔고(balance): **6,254 → 중상위 자산**  
  - loan 비율 낮음 (7.7%), housing loan 46.7%  
  - 직업군: white_collar 53.4%, blue_collar 23.0%  
  - marital: 기혼 64.2%  

- 콜 관련 특성
  - 평균 campaign: 2.66 (소수 차수 콜에 반응)  
  - pdays_no_contact: 0.785 (과거 접촉 경험 多)  
  - poutcome: success 5.2% (세 군집 중 최고)  
  - contact: cellular 66.8%  

- 인사이트
  - 소수 콜에도 반응률이 높은 핵심 타깃 (가입률 17.1%, 최고)  
  - 자산 수준 ↑, loan 부담 ↓ → 예·적금 상품 수용 여력 큼  
  - **전략:** 정기예금·자동이체 유도 + cellular 기반 소수 콜 마케팅 집중  

**2. Cluster 1** (40,159명, 88.8%)

- 재무/고객 특성
  - 평균 나이: 40.5세  
  - 평균 잔고(balance): **632 → 자산 수준 최저**  
  - loan 비율 17.1% (부채 집중), housing loan 57%  
  - 직업군: white_collar 28.8%, blue_collar 31.9%  
  - marital: 기혼 59.6%  

- 콜 관련 특성
  - 평균 campaign: 2.77 (횟수 ↑에도 반응 낮음)  
  - pdays_no_contact: 0.820 (과거 접촉 경험 多)  
  - poutcome: success 3.1% (낮음)  
  - contact: cellular 64.7%  

- 인사이트
  - **콜 효율 최저 군집** (가입률 11.1%)  
  - 자산 ↓ + loan ↑ → 신규 예금 상품 반응 약함  
  - **전략:** 무작위 콜 지양, 대신 부채 관리·대출 상품 전환 제안  

**3. Cluster 2** (306명, 0.7%)

- 재무/고객 특성
  - 평균 나이: 45.6세  
  - 평균 잔고(balance): **25,556 → 고액 자산가**  
  - loan 비율 5.6% (매우 낮음), housing loan 40%  
  - 직업군: white_collar 57.5%, self_employed 13.1%  
  - marital: 기혼 65.4%  

- 콜 관련 특성
  - 평균 campaign: 2.73 (보통 수준 반응)  
  - pdays_no_contact: 0.827  
  - poutcome: success 2.9% (낮음)  
  - contact: cellular 67%  

- 인사이트
  - 소수의 **고액 자산가 군집** (가입률 12.4%)  
  - 일반 텔레마케팅보다 VIP 관리 채널이 적합  
  - **전략:** 자산관리·투자상품 중심, 맞춤형 고액 고객 서비스 제공  

**종합 결론**
- **Cluster 0:** 집중 타깃 (가입률 ↑, 자산·콜 효율 양호)  
- **Cluster 1:** 대규모 집단이지만 비효율 → 무작위 콜 낭비의 주요 원인  
- **Cluster 2:** 소수 VIP 고객 → 별도 자산관리 채널 활용

## 가입여부와 클러스터의 유의성 검정
"""

# y 이진화, 분석 대상 필터
df["y_bin"] = (df["y"].astype(str).str.strip().str.lower() == "yes").astype(int)
mask = df["cluster"].notna() & df["y_bin"].notna()
d = df.loc[mask, ["cluster", "y_bin"]].copy()
d["cluster"] = d["cluster"].astype(int)

# 교차표 + 카이제곱 검정
ct = pd.crosstab(d["cluster"], d["y_bin"])    # counts
row_pct = pd.crosstab(d["cluster"], d["y_bin"], normalize="index").round(3)  # 비율

chi2, p, dof, expected = chi2_contingency(ct)
phi2 = chi2 / ct.values.sum()
r, k = ct.shape

print("[교차표: 건수]")
print(ct)
print("\n[교차표: 행 기준 비율]")
print(row_pct)
print(f"\n[Chi-square] chi2={chi2:.3f}, dof={dof}, p-value={p:.3g}")

# 로지스틱 회귀: y ~ C(cluster)
import statsmodels.formula.api as smf
m = smf.logit("y_bin ~ C(cluster)", data=d).fit(disp=0)
print("\n[Logit: y_bin ~ C(cluster)]")
print(f"LR stat={m.llr:.3f}, LR p-value={m.llr_pvalue:.3g}")

# 군집별 오즈비(기준군은 가장 작은 cluster 번호)
odds = np.exp(m.params).rename("odds_ratio")
ci = np.exp(m.conf_int()).rename(columns={0:"ci_low", 1:"ci_high"})
odds_tbl = pd.concat([odds, ci], axis=1).round(3)
print("\n[오즈비(기준군 대비)]")
print(odds_tbl)

# 군집쌍별 가입률 차이 z-검정
from statsmodels.stats.proportion import proportions_ztest
rates = row_pct[1]  # y=1 비율
counts = ct[1].to_numpy()
ns = ct.sum(1).to_numpy()
clusters = ct.index.to_list()

print("\n[쌍별 z-검정 (Bonferroni 보정 전 p-value)]")
for i in range(len(clusters)):
    for j in range(i+1, len(clusters)):
        stat, pz = proportions_ztest([counts[i], counts[j]], [ns[i], ns[j]])
        print(f"{clusters[i]} vs {clusters[j]}: z={stat:.2f}, p={pz:.3g}  | rates {rates.iloc[i]:.3f} vs {rates.iloc[j]:.3f}")

"""1. 전체 유의성
- p=2.44e-31-> 통계적으로 군집에 따라 가입률이 유의미하게 다름

2. 로지스틱 회귀(y ~ 군집)
- C1 OR = 0.60: C1은 C0 대비 가입 확률이 약 40% 낮음
- C2 OR = 0.69: C2는 C0 대비 가입 확률이 약 31% 낮음
- LR test: p=2.15e-28 -> 모델 전체 유의
- *사후검정 z-test 결과도 C0과 타 클러스터는 유의한 차이를 보임

**종합 결론**
- C0은 훌륭한 타겟층이당....

**클러스터별 공통점과 차이점**

1. 공통점
- 연령대: 세 군집 모두 평균 나이가 40대 초중반(40.5~45.6)
- 연락 채널: cellular 비중이 가장 높음(약 65~67%)
- 과거 접촉 경험: pdays_no_contact 0.78~0.83으로 비슷함
- 결혼 상태: 기혼 비중이 60% 안정적인 가족 기반 고객 다수

2. 차이점
- 자산 수준
  - Cluster0: 중상위 자산층 (평균 6,254)
  - Cluster1: 최저 자산층 (평균 632)
  - Cluster2: 고액 자산가 (퍙균 25,556)
- 부채 상태
  - Cluster 0: loan 비율 낮음 (7.7%), housing loan 47%
  - Cluster 1: loan 비율 높음 (17.1%), housing loan 57% → 부채 집중
  - Cluster 2: loan 비율 매우 낮음 (5.6%), housing loan 40% → 재무적으로 여유
- 가입률
  - Cluster 0: 17.1% (가장 높음)
  - Cluster 1: 11.1% (가장 낮음)
  - Cluster 2: 12.4% (중간, 그러나 인원 적음)
- 직업군
  - Cluster 0: white_collar + blue_collar 혼합, white_collar 우세
  - Cluster 1: white_collar(28.8%) ≈ blue_collar(31.9%) → 블루칼라 집중
  - Cluster 2: white_collar 57.5%, self_employed 13.1% → 화이트칼라·자영업 고자산층

### 무작위 콜 대비 성과
"""

base = df["y_bin"].mean()
byc = df.groupby("cluster")["y_bin"].agg(["mean","count"]).rename(columns={"mean":"conv"})
byc["lift"] = (byc["conv"] / base).round(2)
print("\n[무작위 대비 리프트]\n", byc)

"""특정 클러스터를 타겟팅했을 때 무작위 콜 대비 성과가 얼마나 개선되는지\
lift: 1보다 크면 무작위보다 효율적, 1보다 작으면 비효율적
- Cluster 0: 가입률 17.1%로 전체 평균(11.7% 가량)보다 1.46배 높음 → 핵심 타깃
- Cluster 1: 가입률 11.1%로 평균보다 낮음 (lift 0.95) → 무작위 콜보다 비효율
- Cluster 2: 가입률 12.4%, lift 1.06 → 무작위 대비 약간 우위, 하지만 규모(306명)가 작음

# 결론

궁극적으로, 은행에서 콜 마케팅을 하는 이유는 수익 창출을 위한 것
단순히 클러스터별 콜 효율만 비교해서 클러스터0에 해당하는 고객들을 집중 타겟하자!라는 것 이상의 마케팅 정책 제안이 필요

1. 클러스터 0
- 현재 상품(정기예금) 콜 집중 타겟
- 짧고 집중적인 캠페인으로 효과 극대화

2. 클러스터 1
- 콜 자원 최소화 + 다른 마케팅 방식으로 관리
- 저비용 SMS/앱 푸시
- 메시지 내용: 단순 정기 상품 제안보다 부채 관리, 소액 적립 상품 안내
- 상품 차별화: 마이너스 통장 상환 소액 저축 등 리스크 완화 + 장기 관계 유지 방향

3. 클러스터 2
- 일반 콜 효율 보통, 고객군 규모도 작음
- 소규모 고객군+고액 자산가 고객층인만큼 무작위 콜보다는 선별적·개별적 컨택
- 상품 제안: 단순 예적금보다는 투자/자산관리 상품
"""